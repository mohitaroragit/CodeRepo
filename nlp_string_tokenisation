from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import string
eng_stopwords=stopwords.words("english")
documents=[
    "Yesterday, I was watching movie which was the most horrible one ",
    "then I started to learn python programming",
    "Python programming is used to implement Data Science models"
]
table=str.maketrans("","",string.punctuation)
for i in range(len(documents)):
    documents[i]=documents[i].translate(table)
print(documents)
for i in range(len(documents)):
    documents[i]=word_tokenize(documents[i].lower())
print(documents)
words=[]
for i in range(len(documents)):
    token_list=[]
    for token in documents[i]:
        if token not in eng_stopwords:
            token_list.append(token)
    words.append(token_list)
print(words)
stem=PorterStemmer()
wnet=WordNetLemmatizer()
for i in range(len(words)):
    for j in range(len(words[i])):
        words[i][j]=wnet.lemmatize(words[i][j],pos='v')
print(words)
for i in range(len(words)):
    words[i]=" ".join(words[i])
print(words)
